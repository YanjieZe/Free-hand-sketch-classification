{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.arguments import parse_args\n",
    "from utils.dataloader import DatasetQuickdraw\n",
    "import utils.misc as misc\n",
    "from algorithms import create_model\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data loading: 100%|██████████| 25/25 [00:05<00:00,  4.22it/s]\n",
      "Data loading:   4%|▍         | 1/25 [00:00<00:03,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mData: Loaded 1750000 samples\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Data loading: 100%|██████████| 25/25 [00:01<00:00, 16.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mData: Loaded 62500 samples\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "misc.seed_all(0)\n",
    "transform = None\n",
    "dataset_train = DatasetQuickdraw('quickdraw_png', transform, mode=\"train\")\n",
    "dataset_val = DatasetQuickdraw('quickdraw_png', transform, mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self,C=1.0):\n",
    "        self.C = C\n",
    "        self.W = 0\n",
    "        self.b = 0\n",
    "        \n",
    "    def hingeLoss(self,W,b,X,Y):\n",
    "        loss  = 0.0\n",
    "        \n",
    "        loss += .5*np.dot(W,W.T)\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        \n",
    "        for i in range(m):\n",
    "            ti = Y[i]*(np.dot(W,X[i].T)+b)\n",
    "            loss += self.C *max(0,(1-ti))\n",
    "            \n",
    "        return loss[0][0]\n",
    "    \n",
    "    def fit(self,X,Y,batch_size=50,learning_rate=0.001,maxItr=500):\n",
    "        \n",
    "        no_of_features = X.shape[1]\n",
    "        no_of_samples = X.shape[0]\n",
    "        \n",
    "        n = learning_rate\n",
    "        c = self.C\n",
    "        \n",
    "        #Init the model parameters\n",
    "        W = np.zeros((1,no_of_features))\n",
    "        bias = 0\n",
    "        \n",
    "        #Initial Loss\n",
    "        \n",
    "        #Training from here...\n",
    "        # Weight and Bias update rule\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(maxItr):\n",
    "            #Training Loop\n",
    "            \n",
    "            l = self.hingeLoss(W,bias,X,Y)\n",
    "            losses.append(l)\n",
    "            ids = np.arange(no_of_samples)\n",
    "            np.random.shuffle(ids)\n",
    "            \n",
    "            #Batch Gradient Descent(Paper) with random shuffling\n",
    "            for batch_start in range(0,no_of_samples,batch_size):\n",
    "                #Assume 0 gradient for the batch\n",
    "                gradw = 0\n",
    "                gradb = 0\n",
    "                \n",
    "                #Iterate over all examples in the mini batch\n",
    "                for j in range(batch_start,batch_start+batch_size):\n",
    "                    if j<no_of_samples:\n",
    "                        i = ids[j]\n",
    "                        ti =  Y[i]*(np.dot(W,X[i].T)+bias)\n",
    "                        \n",
    "                        if ti>1:\n",
    "                            gradw += 0\n",
    "                            gradb += 0\n",
    "                        else:\n",
    "                            gradw += c*Y[i]*X[i]\n",
    "                            gradb += c*Y[i]\n",
    "                            \n",
    "                #Gradient for the batch is ready! Update W,B\n",
    "                W = W - n*W + n*gradw\n",
    "                bias = bias + n*gradb\n",
    "                \n",
    "        \n",
    "        self.W = W\n",
    "        self.b = bias\n",
    "        return W,bias,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.array(dataset_train.data, dtype='float32')/255.0\n",
    "label_train= np.array(dataset_train.labels)\n",
    "data_test = np.array(dataset_val.data, dtype='float32')/255.0\n",
    "label_test = np.array(dataset_val.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "combined = list(zip(data_train, label_train))\n",
    "random.shuffle(combined)\n",
    "data_train[:], label_train[:] = zip(*combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = list(zip(data_test, label_test))\n",
    "random.shuffle(combine)\n",
    "data_test[:], label_test[:] = zip(*combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1750000, 784)\n",
      "(1750000,)\n"
     ]
    }
   ],
   "source": [
    "M = data_train.shape[0]\n",
    "data_train = data_train.reshape(M,-1)\n",
    "print(data_train.shape)\n",
    "print(label_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62500, 784)\n",
      "(62500,)\n"
     ]
    }
   ],
   "source": [
    "N = data_test.shape[0]\n",
    "data_test = data_test.reshape(N, -1)\n",
    "print(data_test.shape)\n",
    "print(label_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_classes = len(np.unique(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classWiseData(x, y):\n",
    "    data = {}\n",
    "    \n",
    "    for i in range(number_of_classes):\n",
    "        data[i] = []\n",
    "        \n",
    "    for i in range(x.shape[0]):\n",
    "        data[y[i]].append(x[i])\n",
    "        \n",
    "    for k in data.keys():\n",
    "        data[k] = np.array(data[k])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = classWiseData(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "70000\n"
     ]
    }
   ],
   "source": [
    "print(data[0].shape[0])\n",
    "print(data[1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataPairForSVM(d1,d2):\n",
    "    \n",
    "    l1,l2 = d1.shape[0], d2.shape[0]\n",
    "    samples = l1+l2\n",
    "    features = d1.shape[1]\n",
    "    \n",
    "    data_pair = np.zeros((samples,features))\n",
    "    data_labels = np.zeros((samples,))\n",
    "    \n",
    "    data_pair[:l1,:] = d1\n",
    "    data_pair[l1:,:] = d2\n",
    "    \n",
    "    data_labels[:l1] = -1\n",
    "    data_labels[l1:] = 1\n",
    "    \n",
    "    return data_pair, data_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySVM = SVM()\n",
    "xp, yp = getDataPairForSVM(data[0], data[1])\n",
    "w,b,loss = mySVM.fit(xp,yp,learning_rate=0.00001,maxItr=1000)\n",
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSVMs(x,y):\n",
    "    svm_classifiers = {}\n",
    "    \n",
    "    for i in range(number_of_classes):\n",
    "        svm_classifiers[i] = {}\n",
    "        for j in range(i+1, number_of_classes):\n",
    "            xpair,ypair = getDataPairForSVM(data[i],data[j])\n",
    "            wts,b,loss = mySVM.fit(xpair, ypair,learning_rate=0.00001,maxItr=1000)\n",
    "            svm_classifiers[i][j] = (wts,b)\n",
    "            \n",
    "            plt.plot(loss)\n",
    "            plt.savefig('{}_{}.png'.format(i,j))\n",
    "            plt.show()\n",
    "            \n",
    "    return svm_classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_classifiers = trainSVMs(data_train, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{}_{}.png'.format(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
